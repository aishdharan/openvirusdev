{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# change these to the directory where you cloned openvirusdev\n",
    "HOME = str(Path.home())\n",
    "OPENVIRUSDEV = HOME + \"/\" + \"workspace/openvirusdev\"\n",
    "PROJECTS = HOME + \"/\" + \"projects\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PMR's projects. \n",
    "IPYNB = OPENVIRUSDEV + \"/\" + \"ipynb\"\n",
    "VIRUS2019 = OPENVIRUSDEV + \"/\" + \"viral2019\"\n",
    "\n",
    "OPENBATTERY = PROJECTS + \"/\" + \"open-battery\"\n",
    "LIION = OPENBATTERY + \"/\" + \"liion\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = VIRUS2019\n",
    "project = LIION\n",
    "os.chdir(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_section_globs():\n",
    "    section_globs = {}\n",
    "\n",
    "    section_globs[\"abstract\"]  = (\"abstract\",   '**/sections/**/*abstract*.xml')\n",
    "    section_globs[\"abstract1\"] = (\"abstract1\"), '**/sections/**/*abstract*/*.xml'\n",
    "    section_globs[\"method\"]    = (\"method\",     '**/sections/**/*method*/*.xml')\n",
    "    section_globs[\"all\"]       = (\"all\",        '**/sections/**/*.xml')\n",
    "    section_globs[\"figure\"]    = (\"figure\",     '**/sections/**/*figure*.xml')\n",
    "    section_globs[\"table\"]     = (\"table\",      '**/sections/**/*table*.xml')\n",
    "    section_globs[\"reflist\"]   = (\"ref-list\",    '**/sections/**/*ref-list*/*.xml')\n",
    "\n",
    "    return section_globs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_globbed_files(directory, file_glob, recurse=True):\n",
    "    \"\"\"\n",
    "    returns a list of files satisfying the file_glob expression\n",
    "    in the context of dir\n",
    "    temporarily changes directory and then resets to current dir\n",
    "    recurses through the directory if recursive = True (default)    \n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    files = glob.glob(file_glob, recursive=recurse)\n",
    "#    print(\"number of \" + file_glob + \" files in \" + directory + \": \" + str(len(files)))\n",
    "    os.chdir(current_dir)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_section_dirs():\n",
    "    section_dirs = get_globbed_files(project, '**/sections')\n",
    "    if (len(section_dirs) == 0):\n",
    "        cmd = \"ami -v -p \" + project + \" section\"\n",
    "        print(\"running: \"+cmd)\n",
    "        ! $cmd\n",
    "        section_dirs = get_globbed_files(project, '**/sections')\n",
    "        print(\"found: \" + section_dirs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_glob_dict0():\n",
    "    return get_glob_dict(get_section_globs())\n",
    "\n",
    "def get_glob_dict(section_globs):\n",
    "    \n",
    "    glob_dict = {}\n",
    "    for key in section_globs:\n",
    "        glob = section_globs[key]\n",
    "        glob_dict[key] = glob\n",
    "        \n",
    "    return glob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_text_contents(text_files):\n",
    "    os.chdir(project)\n",
    "\n",
    "    text_contents = []\n",
    "    for text_file in text_files:\n",
    "        text_filex = open(text_file,mode='r')\n",
    "        text = text_filex.read()\n",
    "        text_filex.close()\n",
    "        text_contents.append(text)\n",
    "\n",
    "    return text_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pm286/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "def remove_xml_punkt_tokenize_stopwords(text_contents, minlen=2, lang='english'):\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    stop_en = set(stopwords.words(lang))\n",
    "    stop_sci = get_stopwords_sci();\n",
    "\n",
    "    # filter words\n",
    "    content_words_list = []\n",
    "    for text_content in text_contents:\n",
    "        # remove (most) XML markup\n",
    "        text_content1 = re.sub('</?[^>]*>', '', text_content)\n",
    "        words = word_tokenize(text_content1)\n",
    "        # remove stopwords, punctuation, and short words\n",
    "        words = [w for w in words \\\n",
    "                  if w.lower() not in stop_en \\\n",
    "                  and w not in stop_sci\\\n",
    "                  and w not in string.punctuation\\\n",
    "                  and not matches_noise(w)\\\n",
    "                  and len(w) >= minlen\n",
    "                 ]\n",
    "\n",
    "        content_words_list.append(words)\n",
    "\n",
    "    content_words = [word for lizt in content_words_list for word in lizt]\n",
    "    return content_words\n",
    "\n",
    "def matches_noise(word):\n",
    "    regex = re.compile(\"([A-Z]\\.)\\|(\\-?\\d+)\")\n",
    "    match = regex.fullmatch(word)\n",
    "    print(\"noise \" + word +\", \"+str(match))\n",
    "    return not regex.fullmatch(word) == None\n",
    "\n",
    "def get_stopwords_sci():\n",
    "    words =['et', 'al', 'J.']\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequencies_all():\n",
    "    for section in get_glob_dict0():\n",
    "        analyze_frequencies0(section)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequencies0(section):\n",
    "    glob_expr = get_glob_dict0()[section][1]\n",
    "    print(\"glob \" + glob_expr)\n",
    "    files = get_globbed_files(project, glob_expr)\n",
    "    analyze_frequencies(files, section)\n",
    "    print(\"files \" + str(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def analyze_frequencies(files, title):\n",
    "    text_contents = read_text_contents(files)\n",
    "    words = remove_xml_punkt_tokenize_stopwords(text_contents)\n",
    "    plot_frequency(words, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_frequency(words, title=\"title\"):\n",
    "    fdist = FreqDist(words)\n",
    "    print(fdist.most_common(30))\n",
    "    fdist.plot(30, title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def stem_lemmatize(words, stemmer=PorterStemmer(), lemmatizer=WordNetLemmatizer()):\n",
    "\n",
    "    stemmed = []\n",
    "    for word in words :\n",
    "        if (stemmer != None):\n",
    "            word1 = stemmer.stem(word)\n",
    "        elif (lemmatizer != None):\n",
    "            word1 = lemmatizer.lemmatize(word)\n",
    "        stemmed.append(word1)\n",
    "    return stemmed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "def chunk(text):    \n",
    "    nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "    # tokenize and POS Tagging before doing chunk\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    chunk = ne_chunk(tags)\n",
    "    return chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def noun_phrase(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    reg = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "    parser = nltk.RegexpParser(reg)\n",
    "    np_tree = parser.parse(tags)\n",
    "    return np_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize():\n",
    "    \"\"\"\n",
    "    not tested\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    #tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "    cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_tokenizer(text):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences=sent_tokenize(text)\n",
    "    print(\"sentences: \"+str(len(sentences)))\n",
    "    for sentence in sentences:\n",
    "        print(\">> \"+sentence+\"\\n..\")\n",
    "        phrases = sentence.split(\"\\n\")\n",
    "        print(\"??\"+str(len(phrases)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfVector():\n",
    "    \"\"\"\n",
    "    not tested\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tf=TfidfVectorizer()\n",
    "    # text_tf= tf.fit_transform(data['Phrase'])\n",
    "    text_tf= tf.fit_transform(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf():\n",
    "    \"\"\"\n",
    "    not tested\n",
    "    \"\"\"\n",
    "    import pandas as pd \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    cv=CountVectorizer() \n",
    "    word_count_vector=cv.fit_transform(sentences)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    # print idf values \n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "\n",
    "    # sort ascending \n",
    "    df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# from scikit-learn\n",
    "# https://scikit-learn.org/stable/auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py\n",
    "# =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "matched: xyz\n",
      "xyz: <re.Match object; span=(0, 3), match='xyz'>\n",
      "xyz: False\n",
      "noise xyz, None\n",
      "noise: xyz\n",
      "===\n",
      "J.: None\n",
      "J.: True\n",
      "noise J., None\n",
      "noise: J.\n",
      "===\n",
      "matched: -10\n",
      "-10: <re.Match object; span=(0, 3), match='-10'>\n",
      "-10: False\n",
      "noise -10, None\n",
      "noise: -10\n",
      "===\n",
      "et: None\n",
      "et: True\n",
      "noise et, None\n",
      "noise: et\n"
     ]
    }
   ],
   "source": [
    "# MAIN ENTRY\n",
    "\n",
    "# analyze_frequencies0('abstract')\n",
    "# analyze_frequencies_all();\n",
    "regex = re.compile('xyz|\\-?\\d+')\n",
    "for a in ['xyz', 'J.', '-10', 'et']:\n",
    "    print(\"===\")\n",
    "    match = regex.fullmatch(a)\n",
    "    if not match == None:\n",
    "        print(\"matched: \" + a)\n",
    "    print(a + \": \" + str(match))\n",
    "    print(a + \": \" + str(match == None))\n",
    "    nmatch = matches_noise(a)\n",
    "    print(\"nmatch \" + str(nmatch))\n",
    "    if not nmatch == None:\n",
    "        print(\"noise: \" + a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
