{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# change these to the directory where you cloned openvirusdev\n",
    "HOME = str(Path.home())\n",
    "OPENVIRUSDEV = HOME + \"/\" + \"workspace/openvirusdev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPYNB = OPENVIRUSDEV + \"/\" + \"ipynb\"\n",
    "VIRUS2019 = OPENVIRUSDEV + \"/\" + \"viral2019\"\n",
    "os.chdir(IPYNB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = VIRUS2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_globbed_files(directory, file_glob, recurse=True):\n",
    "    \"\"\"\n",
    "    returns a list of files satisfying the file_glob expression\n",
    "    in the context of dir\n",
    "    temporarily changes directory and then resets to current dir\n",
    "    recurses through the directory if recursive = True (default)    \n",
    "    \"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    files = glob.glob(file_glob, recursive=recurse)\n",
    "#    print(\"number of \" + file_glob + \" files: \" + str(len(files)))\n",
    "    os.chdir(current_dir)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd !ami -p /Users/pm286/workspace/openvirusdev/viral2019 section\n"
     ]
    }
   ],
   "source": [
    "section_dirs = get_globbed_files(project, 'PMC*/sections')\n",
    "if (len(section_dirs) == 0):\n",
    "    cmd = \"!ami -v -p \" + project + \" section\"\n",
    "#    print(\"cmd \"+cmd)\n",
    "    cmd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of PMC* files: 190\n",
      " ['PMC7395660', 'PMC7492837', 'PMC7264879', 'PMC7545115', 'PMC7414897', 'PMC7535057', 'PMC7544696', 'PMC7513765', 'PMC7558233', 'PMC7568452', 'PMC7458341', 'PMC7564663', 'PMC7493721', 'PMC7054940', 'PMC7545945', 'PMC7343983', 'PMC7434691', 'PMC7361212', 'PMC7557816', 'PMC7554493', 'PMC7429370', 'PMC7589794', 'PMC7309422', 'PMC7393812', 'PMC7564522', 'PMC7587540', 'PMC7539923', 'PMC7566875', 'PMC7366979', 'PMC7451973', 'PMC7500495', 'PMC7369338', 'PMC7551450', 'PMC7412619', 'PMC7535344', 'PMC7506502', 'PMC7548526', 'PMC7506195', 'PMC7510690', 'PMC7252054', 'PMC7537046', 'PMC7326430', 'PMC7462755', 'PMC7444461', 'PMC7414311', 'PMC7309642', 'PMC7587986', 'PMC7454403', 'PMC7595211', 'PMC7578542', 'PMC7566657', 'PMC7532770', 'PMC7461367', 'PMC7204690', 'PMC7520074', 'PMC7449939', 'PMC7545018', 'PMC7533430', 'PMC7473250', 'PMC7234863', 'PMC7556465', 'PMC7198142', 'PMC7265663', 'PMC7315839', 'PMC7144339', 'PMC7595053', 'PMC7521564', 'PMC7543746', 'PMC7589865', 'PMC7301718', 'PMC7529449', 'PMC7443314', 'PMC7386254', 'PMC7522205', 'PMC7431552', 'PMC7454758', 'PMC7214320', 'PMC7567250', 'PMC7500500', 'PMC7591948', 'PMC7313810', 'PMC7589781', 'PMC7412776', 'PMC7521428', 'PMC7505682', 'PMC7567661', 'PMC7568689', 'PMC7487961', 'PMC7448901', 'PMC7571305', 'PMC7573694', 'PMC7554465', 'PMC7399656', 'PMC7546957', 'PMC7444586', 'PMC7584246', 'PMC7574430', 'PMC7579579', 'PMC7375274', 'PMC7502241', 'PMC7472086', 'PMC7546332', 'PMC7585508', 'PMC7562768', 'PMC7358766', 'PMC7404868', 'PMC7320811', 'PMC7172701', 'PMC7478525', 'PMC7556531', 'PMC7518975', 'PMC7228357', 'PMC7513903', 'PMC7455036', 'PMC7499620', 'PMC7523766', 'PMC7302093', 'PMC7598168', 'PMC7454266', 'PMC7498470', 'PMC7269026', 'PMC7561266', 'PMC7458854', 'PMC7600846', 'PMC7542670', 'PMC7194909', 'PMC6961731', 'PMC7553334', 'PMC7531915', 'PMC7492142', 'PMC7518201', 'PMC7572017', 'PMC7434770', 'PMC7505526', 'PMC7533885', 'PMC7581214', 'PMC7526598', 'PMC7568326', 'PMC7567380', 'PMC7467230', 'PMC7418792', 'PMC7588517', 'PMC7323509', 'PMC7561220', 'PMC7537237', 'PMC7499315', 'PMC7543942', 'PMC7561681', 'PMC7569420', 'PMC7439087', 'PMC7314948', 'PMC7546239', 'PMC7484601', 'PMC7466926', 'PMC7527697', 'PMC7428766', 'PMC7553375', 'PMC7350463', 'PMC7514082', 'PMC7481547', 'PMC7544991', 'PMC7543972', 'PMC7582078', 'PMC7537512', 'PMC7454931', 'PMC7395223', 'PMC7362814', 'PMC7530558', 'PMC7397949', 'PMC7537317', 'PMC7505219', 'PMC7393948', 'PMC7532837', 'PMC7509824', 'PMC7490917', 'PMC7511210', 'PMC7337641', 'PMC7454301', 'PMC7424620', 'PMC7569564', 'PMC7493736', 'PMC7556584', 'PMC7558248', 'PMC7573891', 'PMC7375030', 'PMC7510706', 'PMC7392634', 'PMC7191543', 'PMC7474797', 'PMC7473125']\n",
      "file type <class 'list'>\n",
      "abstracts []\n",
      "number of xml text files: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "project = VIRUS2019\n",
    "# os.chdir(project)\n",
    "file_glob = 'PMC*'\n",
    "files = get_globbed_files(project, file_glob)\n",
    "print(\"number of \" + file_glob + \" files: \" + str(len(files)) + \"\\n \" + str(files))\n",
    "print(\"file type \" + str(type(files)))\n",
    "\n",
    "abstract_files = get_globbed_files(project, 'PMC*/sections/abstract/*.xml')\n",
    "print(\"abstracts \" + str(abstract_files))    \n",
    "\n",
    "text_files = get_globbed_files(project, 'PMC*/sections/**/*.xml', recurse=False)\n",
    "print(\"number of xml text files: \" + str(len(text_files)) +\"\\n\" + str(text_files))\n",
    "\n",
    "figure_files = get_globbed_files(project, 'PMC*/sections/**/*figure*.xml', recurse=False)\n",
    "# print(\"number of figure files: \" + str(len(figure_files)) +\"\\n\" + str(figure_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_contents = []\n",
    "for text_file in text_files:\n",
    "    text_filex = open(text_file,mode='r')\n",
    "    text = text_filex.read()\n",
    "    text_filex.close()\n",
    "    text_contents.append(text)\n",
    "    \n",
    "len(text_contents) \n",
    "# text_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "minlen = 2\n",
    "\n",
    "stop_en = set(stopwords.words('english'))\n",
    "print(\"stop_en \"+str(stop_en))\n",
    "\n",
    "# filter words\n",
    "content_words_list = []\n",
    "for text_content in text_contents:\n",
    "    # remove (most) XML markup\n",
    "    text_content = re.sub('</?[^>]*>', '', text_content)\n",
    "    words = word_tokenize(text_content)\n",
    "    # remove stopwords, punctuation, and short words\n",
    "    words = [w for w in words if w.lower() not in stop_en \\\n",
    "              and w not in string.punctuation\\\n",
    "              and len(w) >= minlen\n",
    "             ]\n",
    "        \n",
    "    content_words_list.append(words)\n",
    "\n",
    "content_words = [word for lizt in content_words_list for word in lizt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(content_words)\n",
    "# To find the frequency of top 10 words\n",
    "print(fdist.most_common(30))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "fdist.plot(30,cumulative=True)\n",
    "fdist.plot(30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming usually refers to normalizing words into its base form or root form.\n",
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "\n",
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm1 = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm1 :\n",
    " print(word+ \":\" +lst.stem(word))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "tokens = word_tokenize(text1)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text1)\n",
    "tokens\n",
    "tags = nltk.pos_tag(tokens)\n",
    "reg = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "parser = nltk.RegexpParser(reg)\n",
    "result = parser.parse(tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=sent_tokenize(text)\n",
    "print(\"sentences: \"+str(len(sentences)))\n",
    "for sentence in sentences:\n",
    "    print(\">> \"+sentence+\"\\n..\")\n",
    "    phrases = sentence.split(\"\\n\")\n",
    "    print(\"??\"+str(len(phrases)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "# text_tf= tf.fit_transform(data['Phrase'])\n",
    "text_tf= tf.fit_transform(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_tf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer() \n",
    "word_count_vector=cv.fit_transform(sentences)\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# from scikit-learn\n",
    "# https://scikit-learn.org/stable/auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py\n",
    "# =================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def number_normalizer(tokens):\n",
    "    \"\"\" Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "\n",
    "# exclude 'comp.os.ms-windows.misc'\n",
    "categories = ['alt.atheism', 'comp.graphics',\n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
    "              'comp.windows.x', 'misc.forsale', 'rec.autos',\n",
    "              'rec.motorcycles', 'rec.sport.baseball',\n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n",
    "              'sci.med', 'sci.space', 'soc.religion.christian',\n",
    "              'talk.politics.guns', 'talk.politics.mideast',\n",
    "              'talk.politics.misc', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(categories=categories)\n",
    "print(\">> \"+str(len(newsgroups))+\"/\"+str(type(newsgroups))+\"/\"+str(dir(newsgroups)))\n",
    "# 5/<class 'sklearn.utils.Bunch'>/['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
    "print(\"=====\")\n",
    "print(\">ng> \" \\\n",
    "      +str(newsgroups.data[0]) \\\n",
    "      +\"/\"+str(len(newsgroups.data[0])) \\\n",
    "      +\"/filenames: \"+str(len(newsgroups.filenames)) \\\n",
    "      +\"/\"+newsgroups.filenames[0] \\\n",
    "      +\"/\"+str(len(newsgroups.target)) \\\n",
    "      +\"/\"+str(newsgroups.target[0]) \\\n",
    "      +\"/\"+str(newsgroups.target_names[0]) \\\n",
    "     )\n",
    "\n",
    "y_true = newsgroups.target\n",
    "\n",
    "vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=2)\n",
    "cocluster = SpectralCoclustering(n_clusters=len(categories),\n",
    "                                 svd_method='arpack', random_state=0)\n",
    "kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n",
    "                         random_state=0)\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "print(\"xx \"+vectorizer.fit_transform.__doc__)\n",
    "raw_data = newsgroups.data\n",
    "raw_data = sentences\n",
    "print(\"len: \"+str(len(raw_data))+\"/\"+str(raw_data))\n",
    "\n",
    "X = vectorizer.fit_transform(raw_data)\n",
    "# print(\"X \"+X.shape)\n",
    "\n",
    "print(\"Coclustering...\")\n",
    "start_time = time()\n",
    "cocluster.fit(X)\n",
    "y_cocluster = cocluster.row_labels_\n",
    "print(\"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_cocluster, y_true)))\n",
    "\n",
    "print(\"MiniBatchKMeans...\")\n",
    "start_time = time()\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "print(\"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "    time() - start_time,\n",
    "    v_measure_score(y_kmeans, y_true)))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "def bicluster_ncut(i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = X[rows][:, cols].sum()\n",
    "    cut = (X[row_complement][:, cols].sum() +\n",
    "           X[rows][:, col_complement].sum())\n",
    "    return cut / weight\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i)\n",
    "                       for i in range(len(newsgroups.target_names)))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = cocluster.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # categories\n",
    "    counter = defaultdict(int)\n",
    "    for i in cluster_docs:\n",
    "        counter[document_names[i]] += 1\n",
    "    cat_string = \", \".join(\"{:.0f}% {}\".format(float(c) / n_rows * 100, name)\n",
    "                           for name, c in most_common(counter)[:3])\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = cocluster.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = X[:, cluster_words]\n",
    "    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n",
    "                           word_col[out_of_cluster_docs, :].sum(axis=0))\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(feature_names[cluster_words[i]]\n",
    "                           for i in word_scores.argsort()[:-11:-1])\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(\n",
    "        idx, n_rows, n_cols))\n",
    "    print(\"categories   : {}\".format(cat_string))\n",
    "    print(\"words        : {}\\n\".format(', '.join(important_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
